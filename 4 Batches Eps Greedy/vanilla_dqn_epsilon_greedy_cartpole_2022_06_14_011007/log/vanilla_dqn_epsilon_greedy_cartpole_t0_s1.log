[2022-06-14 01:10:27,950 PID:40466 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'max_frame': 1000, 'max_t': None, 'name': 'CartPole-v0'}
- eval_frequency = 100
- log_frequency = 100
- frame_op = None
- frame_op_len = None
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = None
- num_envs = 1
- name = CartPole-v0
- max_t = 200
- max_frame = 1000
- to_render = False
- is_venv = False
- clock_speed = 1
- clock = <slm_lab.env.base.Clock object at 0x7f3623ddbdd8>
- done = False
- total_reward = nan
- u_env = <TrackReward<TimeLimit<CartPoleEnv<CartPole-v0>>>>
- observation_space = Box(4,)
- action_space = Discrete(2)
- observable_dim = {'state': 4}
- action_dim = 2
- is_discrete = True
[2022-06-14 01:10:28,043 PID:40466 INFO base.py end_init_nets] Initialized algorithm models for lab_mode: train
[2022-06-14 01:10:28,116 PID:40466 INFO base.py __init__] VanillaDQN:
- agent = <slm_lab.agent.Agent object at 0x7f3557b4d470>
- action_pdtype = Argmax
- action_policy = <function epsilon_greedy at 0x7f35632dc268>
- explore_var_spec = {'end_step': 4000,
 'end_val': 0.5,
 'name': 'linear_decay',
 'start_step': 0,
 'start_val': 5.0}
- training_start_step = 32
- gamma = 0.99
- training_batch_iter = 8
- training_iter = 4
- training_frequency = 4
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7f3557b66860>
- net = MLPNet(
  (model): Sequential(
    (0): Linear(in_features=4, out_features=64, bias=True)
    (1): SELU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=64, out_features=2, bias=True)
  )
  (loss_fn): MSELoss()
)
- net_names = ['net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0
)
- lr_scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f3557b66a20>
- global_net = None
[2022-06-14 01:10:28,143 PID:40466 INFO __init__.py __init__] Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 100,
 'experiment': 0,
 'experiment_ts': '2022_06_14_011007',
 'git_sha': '9102ff923d7a3e9c579edc18c6547cce94a7b77a',
 'graph_prepath': 'data/vanilla_dqn_epsilon_greedy_cartpole_2022_06_14_011007/graph/vanilla_dqn_epsilon_greedy_cartpole_t0_s1',
 'info_prepath': 'data/vanilla_dqn_epsilon_greedy_cartpole_2022_06_14_011007/info/vanilla_dqn_epsilon_greedy_cartpole_t0_s1',
 'log_frequency': 100,
 'log_prepath': 'data/vanilla_dqn_epsilon_greedy_cartpole_2022_06_14_011007/log/vanilla_dqn_epsilon_greedy_cartpole_t0_s1',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/vanilla_dqn_epsilon_greedy_cartpole_2022_06_14_011007/model/vanilla_dqn_epsilon_greedy_cartpole_t0_s1',
 'prepath': 'data/vanilla_dqn_epsilon_greedy_cartpole_2022_06_14_011007/vanilla_dqn_epsilon_greedy_cartpole_t0_s1',
 'random_seed': 1655137627,
 'resume': False,
 'rigorous_eval': 0,
 'session': 1,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Argmax',
               'action_policy': 'epsilon_greedy',
               'explore_var_spec': {'end_step': 4000,
                                    'end_val': 0.5,
                                    'name': 'linear_decay',
                                    'start_step': 0,
                                    'start_val': 5.0},
               'gamma': 0.99,
               'name': 'VanillaDQN',
               'training_batch_iter': 8,
               'training_frequency': 4,
               'training_iter': 4,
               'training_start_step': 32},
 'memory': {'batch_size': 32,
            'max_size': 10000,
            'name': 'Replay',
            'use_cer': False},
 'name': 'VanillaDQN',
 'net': {'clip_grad_val': 0.5,
         'gpu': False,
         'hid_layers': [64],
         'hid_layers_activation': 'selu',
         'loss_spec': {'name': 'MSELoss'},
         'lr_scheduler_spec': {'frame': 10000, 'name': 'LinearToZero'},
         'optim_spec': {'lr': 0.01, 'name': 'Adam'},
         'type': 'MLPNet'}}
- name = VanillaDQN
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7f3557b4d470>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7f35671d8a90>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": 5.0,
  "entropy_coef": NaN,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4,)",
  "action_space": "Discrete(2)",
  "observable_dim": {
    "state": 4
  },
  "state_dim": 4,
  "action_dim": 2,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Argmax",
  "ActionPD": "<class 'slm_lab.lib.distribution.Argmax'>",
  "memory": "<slm_lab.agent.memory.replay.Replay object at 0x7f3557b4d5f8>"
}
- algorithm = <slm_lab.agent.algorithm.dqn.VanillaDQN object at 0x7f3557b66b38>
[2022-06-14 01:10:28,177 PID:40466 INFO logger.py info] Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 100,
 'experiment': 0,
 'experiment_ts': '2022_06_14_011007',
 'git_sha': '9102ff923d7a3e9c579edc18c6547cce94a7b77a',
 'graph_prepath': 'data/vanilla_dqn_epsilon_greedy_cartpole_2022_06_14_011007/graph/vanilla_dqn_epsilon_greedy_cartpole_t0_s1',
 'info_prepath': 'data/vanilla_dqn_epsilon_greedy_cartpole_2022_06_14_011007/info/vanilla_dqn_epsilon_greedy_cartpole_t0_s1',
 'log_frequency': 100,
 'log_prepath': 'data/vanilla_dqn_epsilon_greedy_cartpole_2022_06_14_011007/log/vanilla_dqn_epsilon_greedy_cartpole_t0_s1',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/vanilla_dqn_epsilon_greedy_cartpole_2022_06_14_011007/model/vanilla_dqn_epsilon_greedy_cartpole_t0_s1',
 'prepath': 'data/vanilla_dqn_epsilon_greedy_cartpole_2022_06_14_011007/vanilla_dqn_epsilon_greedy_cartpole_t0_s1',
 'random_seed': 1655137627,
 'resume': False,
 'rigorous_eval': 0,
 'session': 1,
 'trial': 0}
- index = 1
- agent = <slm_lab.agent.Agent object at 0x7f3557b4d470>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7f35671d8a90>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7f35671d8a90>
[2022-06-14 01:10:28,193 PID:40466 INFO logger.py info] Running RL loop for trial 0 session 1
[2022-06-14 01:10:28,197 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.01  explore_var: 5  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:12:28,498 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 2  t: 19  wall_t: 120  opt_step: 2720  frame: 100  fps: 0.833333  total_reward: 58  total_reward_ma: 58  loss: 5164.7  lr: 0.009903  explore_var: 4.8875  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:15:00,948 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 5  t: 15  wall_t: 273  opt_step: 6720  frame: 200  fps: 0.732601  total_reward: 40  total_reward_ma: 49  loss: 180663  lr: 0.009803  explore_var: 4.775  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:15:01,120 PID:40466 INFO __init__.py log_metrics] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df metrics] final_return_ma: 49  strength: 27.14  max_strength: 36.14  final_strength: 18.14  sample_efficiency: 0.00832903  training_efficiency: 0.000294513  stability: 0.501937
[2022-06-14 01:17:16,039 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 9  t: 14  wall_t: 408  opt_step: 10720  frame: 300  fps: 0.735294  total_reward: 20  total_reward_ma: 39.3333  loss: 1.19844e+07  lr: 0.009703  explore_var: 4.6625  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:17:16,231 PID:40466 INFO __init__.py log_metrics] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df metrics] final_return_ma: 39.3333  strength: 17.4733  max_strength: 36.14  final_strength: -1.86  sample_efficiency: 0.0085063  training_efficiency: 0.000301653  stability: 0.299926
[2022-06-14 01:20:13,791 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 15  t: 10  wall_t: 585  opt_step: 14720  frame: 400  fps: 0.683761  total_reward: 14  total_reward_ma: 33  loss: 1.67704e+07  lr: 0.009603  explore_var: 4.55  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:20:14,025 PID:40466 INFO __init__.py log_metrics] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df metrics] final_return_ma: 33  strength: 11.14  max_strength: 36.14  final_strength: -7.86  sample_efficiency: 0.00956575  training_efficiency: 0.000342879  stability: 0.160626
[2022-06-14 01:20:35,392 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 20  t: 17  wall_t: 607  opt_step: 18720  frame: 500  fps: 0.823723  total_reward: 24  total_reward_ma: 31.2  loss: 3.62179e+07  lr: 0.009503  explore_var: 4.4375  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:20:35,401 PID:40466 INFO __init__.py log_metrics] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df metrics] final_return_ma: 31.2  strength: 9.34  max_strength: 36.14  final_strength: 2.14  sample_efficiency: 0.00921906  training_efficiency: 0.000329615  stability: 0.0125673
[2022-06-14 01:20:57,957 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 26  t: 5  wall_t: 630  opt_step: 22720  frame: 600  fps: 0.952381  total_reward: 14  total_reward_ma: 28.3333  loss: 1.9213e+07  lr: 0.009403  explore_var: 4.325  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:20:57,966 PID:40466 INFO __init__.py log_metrics] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df metrics] final_return_ma: 28.3333  strength: 6.47333  max_strength: 36.14  final_strength: -7.86  sample_efficiency: 0.0107474  training_efficiency: 0.000387411  stability: -0.156317
[2022-06-14 01:21:16,998 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 31  t: 2  wall_t: 649  opt_step: 26720  frame: 700  fps: 1.07858  total_reward: 25  total_reward_ma: 27.8571  loss: 3.423e+07  lr: 0.009303  explore_var: 4.2125  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:21:17,017 PID:40466 INFO __init__.py log_metrics] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df metrics] final_return_ma: 27.8571  strength: 5.99714  max_strength: 36.14  final_strength: 3.14  sample_efficiency: 0.0100504  training_efficiency: 0.000361233  stability: -0.390319
[2022-06-14 01:21:37,986 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 36  t: 10  wall_t: 670  opt_step: 30720  frame: 800  fps: 1.19403  total_reward: 11  total_reward_ma: 25.75  loss: 6.30405e+07  lr: 0.009203  explore_var: 4.1  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:21:37,994 PID:40466 INFO __init__.py log_metrics] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df metrics] final_return_ma: 25.75  strength: 3.89  max_strength: 36.14  final_strength: -10.86  sample_efficiency: 0.0131215  training_efficiency: 0.000475934  stability: -0.619819
[2022-06-14 01:21:56,200 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 39  t: 31  wall_t: 688  opt_step: 34720  frame: 900  fps: 1.30814  total_reward: 15  total_reward_ma: 24.5556  loss: 1.77458e+07  lr: 0.009103  explore_var: 3.9875  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:21:56,209 PID:40466 INFO __init__.py log_metrics] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df metrics] final_return_ma: 24.5556  strength: 2.69555  max_strength: 36.14  final_strength: -6.86  sample_efficiency: 0.0165177  training_efficiency: 0.000602369  stability: -1.18509
[2022-06-14 01:22:11,316 PID:40466 INFO __init__.py log_summary] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df] epi: 44  t: 8  wall_t: 703  opt_step: 38720  frame: 1000  fps: 1.42248  total_reward: 22  total_reward_ma: 24.3  loss: 8.24176e+06  lr: 0.009003  explore_var: 3.875  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-06-14 01:22:11,324 PID:40466 INFO __init__.py log_metrics] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [train_df metrics] final_return_ma: 24.3  strength: 2.44  max_strength: 36.14  final_strength: 0.139999  sample_efficiency: 0.0164286  training_efficiency: 0.000599061  stability: -1.80297
[2022-06-14 01:22:12,906 PID:40466 INFO __init__.py log_metrics] Trial 0 session 1 vanilla_dqn_epsilon_greedy_cartpole_t0_s1 [eval_df metrics] final_return_ma: 24.3  strength: 2.44  max_strength: 36.14  final_strength: 0.139999  sample_efficiency: 0.0164286  training_efficiency: 0.000599061  stability: -1.80297
[2022-06-14 01:22:12,907 PID:40466 INFO logger.py info] Session 1 done
